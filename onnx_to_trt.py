import pickle
import argparse
import tensorrt as trt

# logger to capture errors, warnings, and other information during the build and inference phases
TRT_LOGGER = trt.Logger()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        '--onnx',
        type=str,
        help="Path of onnx model generated by 'torch_to_onnx.py'.",
        required=True
    )
    parser.add_argument(
        '--engine',
        type=str,
        help="Path for saving tensorrt engine.",
        required=True
    )

    args = parser.parse_args()

    onnx_file_path = args.onnx
    # initialize TensorRT engine and parse ONNX model
    EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)

    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(EXPLICIT_BATCH)
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # parse ONNX
    with open(onnx_file_path, 'rb') as model:
        print('Beginning ONNX file parsing')
        parser.parse(model.read())
    print('Completed parsing of ONNX file')

    # allow TensorRT to use up to 1GB of GPU memory for tactic selection
    builder.max_workspace_size = 1 << 30
    # we have only one image in batch
    builder.max_batch_size = 1
    # use FP16 mode if possible
    if builder.platform_has_fast_fp16:
        builder.fp16_mode = True

    # generate TensorRT engine optimized for the target platform
    print('Building an engine...')
    engine = builder.build_cuda_engine(network)
    print("Completed creating Engine")

    with open(args.engine, 'wb') as f:
        f.write(engine.serialize())
